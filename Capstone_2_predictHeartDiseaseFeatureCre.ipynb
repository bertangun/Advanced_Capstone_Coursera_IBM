{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis and Prediction of Heart Disease\n",
    "### (Feature Creation)\n",
    "## 3. Feature Creation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records =  303 \n",
      "\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
      "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|     1|\n",
      "| 56|  1|  1|     120| 236|  0|      1|    178|    0|    0.8|    2|  0|   2|     1|\n",
      "| 57|  0|  0|     120| 354|  0|      1|    163|    1|    0.6|    2|  0|   2|     1|\n",
      "| 57|  1|  0|     140| 192|  0|      1|    148|    0|    0.4|    1|  0|   1|     1|\n",
      "| 56|  0|  1|     140| 294|  0|      0|    153|    0|    1.3|    1|  0|   2|     1|\n",
      "| 44|  1|  1|     120| 263|  0|      1|    173|    0|    0.0|    2|  0|   3|     1|\n",
      "| 52|  1|  2|     172| 199|  1|      1|    162|    0|    0.5|    2|  0|   3|     1|\n",
      "| 57|  1|  2|     150| 168|  0|      1|    174|    0|    1.6|    2|  0|   2|     1|\n",
      "| 54|  1|  0|     140| 239|  0|      1|    160|    0|    1.2|    2|  0|   2|     1|\n",
      "| 48|  0|  2|     130| 275|  0|      1|    139|    0|    0.2|    2|  0|   2|     1|\n",
      "| 49|  1|  1|     130| 266|  0|      1|    171|    0|    0.6|    2|  0|   2|     1|\n",
      "| 64|  1|  3|     110| 211|  0|      0|    144|    1|    1.8|    1|  0|   2|     1|\n",
      "| 58|  0|  3|     150| 283|  1|      0|    162|    0|    1.0|    2|  0|   2|     1|\n",
      "| 50|  0|  2|     120| 219|  0|      1|    158|    0|    1.6|    1|  0|   2|     1|\n",
      "| 58|  0|  2|     120| 340|  0|      1|    172|    0|    0.0|    2|  0|   2|     1|\n",
      "| 66|  0|  3|     150| 226|  0|      1|    114|    0|    2.6|    0|  0|   2|     1|\n",
      "| 43|  1|  0|     150| 247|  0|      1|    171|    0|    1.5|    2|  0|   2|     1|\n",
      "| 69|  0|  3|     140| 239|  0|      1|    151|    0|    1.8|    2|  2|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-39b8034e-49e4-4e3e-b6fd-44f01e7f35b9',\n",
    "    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'api_key': 'iN3J17eRe8Mezyc57J4mtxTRFl-oey7Re0eCAwWgP_8T'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_0dc47b3a85024faf8c9e361e756597e5_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data = spark.read.parquet(cos.url('heart.parquet', 'definition-donotdelete-pr-cluwtgruaxvlbz'))\n",
    "print(\"Number of records = \", df_data.count(), \"\\n\")\n",
    "df_data.createOrReplaceTempView('df')\n",
    "df_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Feature Engineering :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply One Hot Encoding to the categorical integer features.These are sex, cp, fbs, restecg, exang, slope, ca ve thal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Create one hot encoders for the categorical features\n",
    "encoder1 = OneHotEncoder(inputCol = 'sex', outputCol = 'sexVec')\n",
    "encoder2 = OneHotEncoder(inputCol = 'cp', outputCol = 'cpVec')\n",
    "encoder3 = OneHotEncoder(inputCol = 'fbs', outputCol = 'fbsVec')\n",
    "encoder4 = OneHotEncoder(inputCol = 'restecg', outputCol = 'restecgVec')\n",
    "encoder5 = OneHotEncoder(inputCol = 'exang', outputCol = 'exangVec')\n",
    "encoder6 = OneHotEncoder(inputCol = 'slope', outputCol = 'slopeVec')\n",
    "encoder7 = OneHotEncoder(inputCol = 'ca', outputCol = 'caVec')\n",
    "encoder8 = OneHotEncoder(inputCol = 'thal', outputCol = 'thalVec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the features into a single features vector and then normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# Create a features vector\n",
    "vectorAssembler = VectorAssembler(inputCols = ['sexVec', 'cpVec', 'trestbps', 'chol', 'fbsVec', 'restecgVec', \n",
    "                                               'thalach', 'exangVec', 'oldpeak', 'slopeVec', 'caVec','thalVec'],\n",
    "                                  outputCol = 'featuresVec')\n",
    "\n",
    "# Normalize the features data\n",
    "normalizer = MinMaxScaler(inputCol = 'featuresVec', outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------------+--------------------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|       sexVec|        cpVec|       fbsVec|   restecgVec|     exangVec|     slopeVec|        caVec|      thalVec|         featuresVec|            features|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------------+--------------------+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|    (1,[],[])|    (3,[],[])|    (1,[],[])|(2,[0],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(4,[0],[1.0])|(3,[1],[1.0])|(21,[4,5,7,9,10,1...|[0.0,0.0,0.0,0.0,...|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|    (1,[],[])|(3,[2],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[3,4,5,6,8,9,...|[0.0,0.0,0.0,1.0,...|\n",
      "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|     1|(1,[0],[1.0])|(3,[1],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[0,2,4,5,6,7,...|[1.0,0.0,1.0,0.0,...|\n",
      "| 56|  1|  1|     120| 236|  0|      1|    178|    0|    0.8|    2|  0|   2|     1|    (1,[],[])|(3,[1],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[2,4,5,6,8,9,...|[0.0,0.0,1.0,0.0,...|\n",
      "| 57|  0|  0|     120| 354|  0|      1|    163|    1|    0.6|    2|  0|   2|     1|(1,[0],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|    (1,[],[])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[0,1,4,5,6,8,...|[1.0,1.0,0.0,0.0,...|\n",
      "| 57|  1|  0|     140| 192|  0|      1|    148|    0|    0.4|    1|  0|   1|     1|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(4,[0],[1.0])|(3,[1],[1.0])|(21,[1,4,5,6,8,9,...|[0.0,1.0,0.0,0.0,...|\n",
      "| 56|  0|  1|     140| 294|  0|      0|    153|    0|    1.3|    1|  0|   2|     1|(1,[0],[1.0])|(3,[1],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[0,2,4,5,6,7,...|[1.0,0.0,1.0,0.0,...|\n",
      "| 44|  1|  1|     120| 263|  0|      1|    173|    0|    0.0|    2|  0|   3|     1|    (1,[],[])|(3,[1],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|    (3,[],[])|(21,[2,4,5,6,8,9,...|[0.0,0.0,1.0,0.0,...|\n",
      "| 52|  1|  2|     172| 199|  1|      1|    162|    0|    0.5|    2|  0|   3|     1|    (1,[],[])|(3,[2],[1.0])|    (1,[],[])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|    (3,[],[])|(21,[3,4,5,8,9,10...|[0.0,0.0,0.0,1.0,...|\n",
      "| 57|  1|  2|     150| 168|  0|      1|    174|    0|    1.6|    2|  0|   2|     1|    (1,[],[])|(3,[2],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[3,4,5,6,8,9,...|[0.0,0.0,0.0,1.0,...|\n",
      "| 54|  1|  0|     140| 239|  0|      1|    160|    0|    1.2|    2|  0|   2|     1|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[1,4,5,6,8,9,...|[0.0,1.0,0.0,0.0,...|\n",
      "| 48|  0|  2|     130| 275|  0|      1|    139|    0|    0.2|    2|  0|   2|     1|(1,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[0,3,4,5,6,8,...|[1.0,0.0,0.0,1.0,...|\n",
      "| 49|  1|  1|     130| 266|  0|      1|    171|    0|    0.6|    2|  0|   2|     1|    (1,[],[])|(3,[1],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[2,4,5,6,8,9,...|[0.0,0.0,1.0,0.0,...|\n",
      "| 64|  1|  3|     110| 211|  0|      0|    144|    1|    1.8|    1|  0|   2|     1|    (1,[],[])|    (3,[],[])|(1,[0],[1.0])|(2,[0],[1.0])|    (1,[],[])|(2,[1],[1.0])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[4,5,6,7,9,11...|[0.0,0.0,0.0,0.0,...|\n",
      "| 58|  0|  3|     150| 283|  1|      0|    162|    0|    1.0|    2|  0|   2|     1|(1,[0],[1.0])|    (3,[],[])|    (1,[],[])|(2,[0],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[0,4,5,7,9,10...|[1.0,0.0,0.0,0.0,...|\n",
      "| 50|  0|  2|     120| 219|  0|      1|    158|    0|    1.6|    1|  0|   2|     1|(1,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[0,3,4,5,6,8,...|[1.0,0.0,0.0,1.0,...|\n",
      "| 58|  0|  2|     120| 340|  0|      1|    172|    0|    0.0|    2|  0|   2|     1|(1,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[0,3,4,5,6,8,...|[1.0,0.0,0.0,1.0,...|\n",
      "| 66|  0|  3|     150| 226|  0|      1|    114|    0|    2.6|    0|  0|   2|     1|(1,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[0,4,5,6,8,9,...|[1.0,0.0,0.0,0.0,...|\n",
      "| 43|  1|  0|     150| 247|  0|      1|    171|    0|    1.5|    2|  0|   2|     1|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[0],[1.0])|(3,[2],[1.0])|(21,[1,4,5,6,8,9,...|[0.0,1.0,0.0,0.0,...|\n",
      "| 69|  0|  3|     140| 239|  0|      1|    151|    0|    1.8|    2|  2|   2|     1|(1,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|(4,[2],[1.0])|(3,[2],[1.0])|(21,[0,4,5,6,8,9,...|[1.0,0.0,0.0,0.0,...|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a Feature Engineering ML pipeline\n",
    "pipeline = Pipeline(stages = [encoder1, encoder2, encoder3, encoder4, encoder5, encoder6, encoder7, encoder8, vectorAssembler, normalizer])\n",
    "df_normalized_data = pipeline.fit(df_data).transform(df_data)\n",
    "df_normalized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the normalized features, let's drop all the other columns from our dataset and features vector except target and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|target|            features|\n",
      "+------+--------------------+\n",
      "|     1|[0.0,0.0,0.0,0.0,...|\n",
      "|     1|[0.0,0.0,0.0,1.0,...|\n",
      "|     1|[1.0,0.0,1.0,0.0,...|\n",
      "|     1|[0.0,0.0,1.0,0.0,...|\n",
      "|     1|[1.0,1.0,0.0,0.0,...|\n",
      "|     1|[0.0,1.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,1.0,0.0,...|\n",
      "|     1|[0.0,0.0,1.0,0.0,...|\n",
      "|     1|[0.0,0.0,0.0,1.0,...|\n",
      "|     1|[0.0,0.0,0.0,1.0,...|\n",
      "|     1|[0.0,1.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,0.0,1.0,...|\n",
      "|     1|[0.0,0.0,1.0,0.0,...|\n",
      "|     1|[0.0,0.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,0.0,1.0,...|\n",
      "|     1|[1.0,0.0,0.0,1.0,...|\n",
      "|     1|[1.0,0.0,0.0,0.0,...|\n",
      "|     1|[0.0,1.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,0.0,0.0,...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_normalized_data = df_normalized_data.drop('age').drop('sex').drop('cp').drop('trestbps').drop('chol') \\\n",
    "                        .drop('fbs').drop('restecg').drop('thalach').drop('exang').drop('oldpeak').drop('slope') \\\n",
    "                        .drop('ca').drop('thal').drop('sexVec').drop('cpVec').drop('fbsVec').drop('restecgVec') \\\n",
    "                        .drop('exangVec').drop('slopeVec').drop('caVec').drop('thalVec').drop('featuresVec')\n",
    "                        \n",
    "df_normalized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Store Feature Engineered Data in IBM Object Store :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this data set(heart_normalized.parquet) our future modeling, prediction and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_data = df_normalized_data.repartition(1)\n",
    "df_normalized_data.write.parquet(cos.url('heart_normalized.parquet','definition-donotdelete-pr-cluwtgruaxvlbz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records persisted =  303\n",
      "+------+--------------------+\n",
      "|target|            features|\n",
      "+------+--------------------+\n",
      "|     1|[0.0,0.0,0.0,0.0,...|\n",
      "|     1|[0.0,0.0,0.0,1.0,...|\n",
      "|     1|[1.0,0.0,1.0,0.0,...|\n",
      "|     1|[0.0,0.0,1.0,0.0,...|\n",
      "|     1|[1.0,1.0,0.0,0.0,...|\n",
      "|     1|[0.0,1.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,1.0,0.0,...|\n",
      "|     1|[0.0,0.0,1.0,0.0,...|\n",
      "|     1|[0.0,0.0,0.0,1.0,...|\n",
      "|     1|[0.0,0.0,0.0,1.0,...|\n",
      "|     1|[0.0,1.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,0.0,1.0,...|\n",
      "|     1|[0.0,0.0,1.0,0.0,...|\n",
      "|     1|[0.0,0.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,0.0,1.0,...|\n",
      "|     1|[1.0,0.0,0.0,1.0,...|\n",
      "|     1|[1.0,0.0,0.0,0.0,...|\n",
      "|     1|[0.0,1.0,0.0,0.0,...|\n",
      "|     1|[1.0,0.0,0.0,0.0,...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_persisted_data = spark.read.parquet(cos.url('heart_normalized.parquet', \n",
    "                                               'definition-donotdelete-pr-cluwtgruaxvlbz'))\n",
    "print('Number of records persisted = ', df_persisted_data.count())\n",
    "df_persisted_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our normalized data set has been successfully written to IBM object storage and, there is not any data loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
